type: "SNIPPET_NOTE"
folder: "5c21c34ed317a94d49a2"
title: "Machine learning knowledge"
description: "Machine learning knowledge"
snippets: [
  {
    name: "MachineLearningKnowledge.py"
    mode: "Python"
    content: '''
      #要約統計量
      要約統計量とは平均や、中央値、分散(バラけ具合)と言った様な、
      データ分析の為に理解する必要がある統計的な数値の事。pandasライブラリーのdescribeメソッドで
      取得することが出来ます。
      
      countはデータの総数で、
      meanは平均。
      stdは標準偏差(データのバラつき具合)を表しています。
      minとmaxはそれぞれテーブル内の値の最大、最小値です。
      25%,50%,75%はパーセント点です。例えば、
      
      25％点の値とはデータを100個並べた時に25番目の大きさの値。
      
      
      
      #ヒストグラム
      データのある特徴量が分布している頻度を可視化するために利用されます。
      
      
      
      #説明変数
      目的変数を説明する為の数値のことを説明変数と言います。
      特定の人物の肥満度を計測したいとしたら、「BMI」が目的変数となり、
      「体重と身長」が説明変数となります。
      
      
      
      #目的変数
      予測したい数値のこと
      
      
      #トレーニングデータ、テストデータ
      機械学習で学習モデルを作成する際は、必ずデータをトレーニング(学習用)データとテストデータに
      分割します。トレーニングデータとテストデータに分けるのは、作成したモデルの評価に必要な為です。
      機械学習の基本は「学習用のデータでモデルを作り、未知のデータの予測・分類をする」事です。
      トレーニングデータとは、モデルにとって学習用のデータでありテストデータとはモデルとって
      未知のデータとなります。
      
      
      
      #線形回帰(Linear regression)
      観測したデータセットに直線をフィットする
      この直線をまだ観測されていない値の予測に用いる 
      
      "どのように用いるのか"
      最小二乗法を用いるのが普通
      最小二乗法は二乗誤差の和を最小化する。最尤推定とも呼ばれる 
      各点と直線の間の二乗距離を最小化する
      一次関数によるフィッティング y = mx+b
      
      線形回帰モデルの数式
      y(目的変数)=ax(説明変数)+b
      
      "R-二乗値 : 別名-決定係数"
      直線がデータにどのくらいフィットしているか
      0-1の範囲
      0はモデルに当てはまりが良くない(直線で予測できない)
      1はモデルによく当てはまる(直線で予測できる)
      "計算式"
      1.0 - 誤差の二乗和/分散の二乗和
      
      
      #多項式回帰
      曲線による近似
      全ての関係が線形ではない
      線形の式 : y = mx+b
        これを一次の多項式
      二次の多項式 : y = ax^2 + bx + c
      三次の多項式 : y = ax^3 + bx^2 +cx + d
      次数の多い多項式はより複雑な曲線を描画できる
      
      "過剰適用に気をつけよう"
      必要以上の次数を使わない
      曲線が実際どの程度複雑になるか確認するために、最初にデータを可視化しよう
      外れ値に適用し、曲線がおかしくなっていないか確認しよう
      R-二乗値が大きいことは、データに曲線がよくフィットしていることを意味する。
      しかしながら、その曲線が必ずしも予測に使えるわけではない。
      
      
      #平均二乗誤差(Mean Squared Error/MSE)
      平均二乗誤差(MSE)とは、ある連続値と連続値の距離を二乗した値の平均の事です。
      回帰分析で登場する残差の全データ平均を計算したいときなどに使用されます。
      残差はプラスに成る時もマイナスになる時もあるため、平均値を取得する際は全ての残差を
      二乗する必要があります。
      
      平均二乗誤差(残差の平均)の値が小さくなればなるほど、そのモデルの精度は高くなるので、
      モデルの精度を測る基準として利用します
      
      もし精度が不十分だった場合は
      ①アルゴリズムを選択し直す
      ②説明変数を絞り込む(Lesson8で具体的な手法を学習)
      ③データ量を増やし、学習量を増やす。などを行い、
        更に精度を上げるために取り組んでいく必要があります。
        
        
        
      #分類とは、データ集合をそれぞれのカテゴリごとに分ける機械学習の代表的な手法です。
      例えば、スパムメールをメールBOXから判別したり、ニュース記事を政治やスポーツ、
      エンタメなどに分けるたり、画像分類などが分類問題です。
      
      
      #二値分類
      非常にシンプルな方法です。データ集合をある特徴量から、2つのグループに分けます。
      綺麗に分類できないときは、特徴量の取り方・次元の操作を繰り返します。
      
      
      #多クラス分類
      二値分類を応用した分類方法で、データ集合を幾つかのグループに分けることが目的です。
      多クラス分類でもっとも基本的な考え方は、「1対その他」というものです。
      複数のクラスがあったとしても、それらを「注目するクラス」と「その他のクラス」として
      分類する手法です。
      
      
      
      #データセットとは
      本来、データを分析する際は、自分が分析したいデータが集計されているCSVファイルなどを使います。
      しかし、機械学習を学習する段階で、毎回データセットを作っていると面倒です。なので、
      機械学習を学ぶ上で、取り扱いやすいように整理された、無料のデータセットを使用することにします。
      
      
      
      #irisデータセット
      このデータセット主な用途は、「分類」です。
      この有名なアヤメのデータセットは三種類のアヤメの品種のそれぞれの50の花のcentimètre単位の
      がく片の長さと幅、花弁の長さと幅の計算結果を与えてくれています。
      品種は「setosa」「versicolor」「virginica」です。
      レコードの数は150行 、カラムの数は5列
      (Sepal Lemgth, Sepal Width, Petal Length, Petal Width, Species)です。
      
      
      
      #ロジスティック回帰
      広く使われている分類モデルの1つである。しかし、高い性能を発揮してくれるのは、
      線形分離可能なクラスに対してのみ。
      名前には「回帰」と入っていますが、分類のためのモデルなので気をつましょう。
      
      
      
      #線形分離可能
      2次元の平面上にデータ点があると仮定すると、そのデータ点を一本の直線で分割できることを表します。
      3次元以上になると、より複雑な処理が必要になるので、
      今回は2次元までを扱います。3次元以上のデータを扱う際は、
      「次元の削除」(Lesson8で詳述)を使って次元を落として、　線形分離する場合もあります。
      
      
      
      #分類
      与えられたクラスにデータを分けていきます。
      例えば、画像分類やスパムメール識別などがあります。
      
      
      
      #回帰
      連続するデータを予測するときなどに使用します。
      そのデータから特徴量を見つけ、座標上に表す。そこから数値を予測します。
      
      
      #決定境界
      クラスを区別するための境界のことを指します。
      
      
      
      #標準化
      生のデータは、項目によって単位が異なったり、極端に値が大きかったりします。
      この生データをそのまま使ってしまうと、それぞれの関係を正しく評価できない可能性があります。
      なので、機械学習においては、最適な性能を得るために特徴量のスケールを整える必要があります。
      その方法が標準化です
      
      
      #過学習
      トレーニングデータに強く依存してしまい、汎化性能が低い状態を指します。
      つまり、トレーニングデータでは分類が正確にできるのに、テストデータでは分類を失敗する
      可能性が高い状態です。
      本来は新しいデータを正しく評価しなくてはならないのに、それが正しく行えない状態なので、
      学習の失敗です。
      実生活でいうと、参考書の練習問題は簡単に解けるのに、試験問題は全く解けない状態と同じ状態です。
      
      
      #正則化
      モデルの複雑さを調整することが可能です。つまり、過学習を防ぐことができます。
      その方法は、極端な値をとるデータにペナルティをかけることにより、影響を少なくします
      
      
      #ハイパーパラメータ
      ほとんどのパラメータは、データの学習を進めることで決めることができます。
      しかし、ハイパーパラメータは、学習によって決めることができません。
      ロジスティック回帰の正則化パラメータは、ハイパーパラメータです。
      このようなハイパーパラメータは、これくらいが適切だとされる値がある場合があります。しかし、
      精度の良いモデルを作成したい場合は、幾つかのハイパーパラメータで学習させて、
      一番結果の良いものを採用することがあります。
      
      
      #グリッドサーチ
      ハイパーパラメータの探索範囲を格子状(グリッド)に区切り、
      交点となるハイパーパラメータの組み合わせをすべて試していく方法です。
      莫大な計算量が必要になることが多いのが、デメリットです。
      
      
      #ベイズ最適化
      初めは無作為にハイパーパラメータを抽出していき探索します。そして、
      その結果を用いて次使うハイパーパラメータを確率的に導いていく方法です。
      すべての点を評価する必要がなく、効率的に最適値にたどり着くことが可能です。
      
      
      
      #数値データ
      定量データの測定 : 身長、ウェブサイトのロード時間、株価など
      離散データ : 整数ベース、イベント数のカウント
        顧客の購買数
        首を振った回数
      連続データ : 連続的な値、とりうる値が無数にある
        ログアウトに要した時間
        １日の降水量
        
        
      #カテゴリデータ
      数学的な意味を内在しないデータ : 性別、Yes/No、人種、居住地、業種、支持政党
      カテゴリーに番号を振る振ることもできるが、それ自体に数学的な意味はない
      
      
      #順序データ
      数値データとカテゴリデータの合いの子
      数学的意味を持ったカテゴリデータ
      映画の五段階評価
      
      
      #平均値
      平均値は合計値をサンプル値で割ったもの
      ex.
      一世帯あたりの平均の子供の数：
      0,2,3,2,1,0,0,2,0
      平均値は(0+2+3+2+1+0+0+2+0)/9 = 1.11
      
      
      #中央値
      値をソートして、中央の値をとる
      サンプル数が偶然なら、中央の２つの平均をとる。
      中央値は平均値よりも外れ値の影響を受けにくい
      ex.
      0,2,3,2,1,0,0,2,0
          ソート
      0,0,0,0,1,2,2,2,3
      
      
      
      #モード
      データセットにおけるもっとも頻度の高い値
        連続数値データには当てはまらない
      一世帯当たりの子供の数の例で
      0,2,3, 2,1,0,0,2,0
          各値の頻度は?
      0:4 , 1:1 , 2:3 , 3:1
      モードは0
      
      
      #分散
      データがどれだけ広がっているかを表す
      (σ^2)シグマ２乗は平均値との差の二乗平均
      データセット(1,4,5,4,8)の分散は？
      1.平均値を計算 : (4.4)
      2.平均値との差を計算する : (-3.4,-0.4,0.6,-0.4,3.6)
      3.差の二乗を計算する : (11.56,0.16,0.36,0.16,12.96)
      4.二乗差の平均を計算する
      σ^2 = (11.56 + 0.16 + 0.36 + 0.16 + 12.96) / 5 = 5.04
      
      
      #共分散
      2組のデータセット間の関係を表す数値
      
      "共分散の計算"
      1.２つの同じ長さのデータセットを用意する
      2.これから、それぞれの平均値を引く
      3.それぞれのデータセットを高次元のベクトルととらえ、内積(対応する各要素の積の総和)を
        計算。
      4.上記をサンプルの長さで割る
      
      "共分散の解釈"
      共分散が0に近いことは２つのデータセットの相関が大きくないことを意味する
      大きな共分散(正でも、負でも)は相関があることを意味してる
      
      "相関の計算"
      共分散をそれぞれのデータセットの標準偏差で割り、正規化する
      -1の相関は完全な逆相関
      相関0は相関なし
      相関1は完全な相関
      
      相関は因果関係を示唆しているわけではない
      相関はどの実験を行うか判断するために用いましょう
      
      
      #標準偏差
      分散の平方根
      外れ値の検出をするときに用いられる。平均値から標準偏差一つ分外れたデータは
      通常ではないと考えられる
      データが極端な値であるかどうかは、平均値平均値から何シグマ分離れているかで判断できる 
      σ^2 = 5.04
      σ =　√5.04 = 2.24 = (1,4,5,4,8)の標準偏差は2.24
      
      
      
      #母分散
      数式
      σ^2 = ∑(x - µ)^2 / N
      σ^2 = (11.56 + 0.16 + 0.36 + 0.16 + 12.96) / 5 = 5.04
      のように計算
      
      
      #不偏標本分散
      数式
      S^2 = ∑(x-M)^2 / N - 1
      S^2 = (11.56 + 0.16 + 0.36 + 0.16 + 12.96) / 4 = 6.3
      のように計算
      
      
      
      #確率密度関数
      ある一つのデータの値が特定の範囲に収まる確率
      グラフだと曲線で連続的な値をとる
      
      #確率質量関数
      離散的な値で表現される 
      
      
      #パーセンタイル
      データセットにおいて、x%の値がその値より下の点
      全体の何%が含まれるかを数値でわかりやすく示したもの
      
      
      #モーメント
      確率密度関数の形状の定量化
      "一次モーメント"
      平均のこと
      "二次モーメント"
      分散のこと
      "三次モーメント"
      歪度<わいど>(V) : 分布がどれだけ偏っているか
      左に長いロングテールを持つ分布は負の歪度を持つ、逆は正の歪度を持つ
      "四次モーメント"
      尖度<せんど> : ピークがどれだけ尖っているか、どれだけ広がっているか
      高いピークは尖度が大きい
      
      
      
      #条件付き確率
      あるイベントが起きたという条件のもとで、もう一つのイベントが起きる確率
      お互いに依存する2つのイベントA,Bを考える
      P(A,B)の表記はAとBが両方ともに起きる確率
      P(B|A)はAが起きた場合にBが起きる確率
      P(B|A) = P(A,B) / P(A)
      
      ex
      生徒に２つの試験を課します。60%の生徒は両方の試験を合格しますが、
      最初の試験は80%の生徒が合格します。
      最初の試験に合格した生徒が２つ目の試験に合格する確率は何％でしょうか？
      
      A = 最初の試験に合格、B = 2つ目の試験に合格
      P(B|A),すなわちAが起きた時Bが起きる確率を求める
      P(B|A) = P(A,B) / P(A) = 0.75
      最初の試験に合格した人が2つ目の試験に合格する確率は75%
      
      
      #ベイズの定理
      "P(A|B) = P(A)P(B|A) / P(B)"
      Bが前提でAが起こる確率は、Aの確率にAが前提でBが起きる確率をかけてBの確率で割った
      値に等しい
      直感に反した確率を得られる
      
      ex
      あるテストがある薬の使用者の99%に陽性を示し、非ユーザーの99%に陰性を示す。
      しかしながら、人口の0.3しか薬を使用者はいない
      
      A = 実際にその薬の使用者かどうか
      B = その薬に陽性反応を示すかどうか
      この場合、P(B)は1.3%(0.99 * 0.003 + 0.01 * 0.997)
      実際に使用者であって陽性を示す確率と、非使用者であって陽性を示す確率の和
      P(A|B) = P(A)P(B|A) / P(B) = 0.003 * 0.99 / 0.013 = 22.8%
      陽性を示した被験者の中で、実際の薬の使用者は22.8%しかいない
      P(B|A)が高くても、それは P(A|B)が高いことを意味しない
      
      
      #多編量回帰(重回帰)
      複数の属性を元に値の予測を行う
      複数の変数が一つの変数に影響を与える場合 
      ex 
      ボディの種類、メーカー、走行距離などによる車の価格予想
      
      各項の係数が重要
      フィットの度合いはR-二乗値で測定できる
      異なる属性はお互いに依存していないという前提が必要
      
      
      
      #マルチレベル
      階層的な構造を持ったデータ分析をする
      ex
      個人の健康 : 細胞、臓器、個人、家族、街、国家などに依存している
      個人の資産 : 仕事、父母の行動、祖父母の行動などに依存している
      マルチレベルモデルはそれらの依存関係をモデル化し、説明することを試みる
      
      "多層のモデル化"
      結果に影響する要因において識別する必要がある
      例えば、センター試験のスコアは遺伝子、家庭環境、居住地の平均所得、教育の質
      地区の教育予算、県の教育方針などに基づいて予測される
      これらの要因は、複数の層に影響を及ぼすこともある。例えば、
      居住地の平均所得は家庭環境 にも影響を与えうる
      
      
      #質的変数
      質的変数とはデータ同士に優劣が無く、大小の比較ができない変数です。
      例えば「チャールズ川に近いかどうか」は優劣はなく、
      どの値を持っていたとしてもデータ同士は対等です。こういった状態を数値化したものを
      質的変数もしくはカテゴリー変数と言います
      
      
      #量的変数
      量的変数とは、データ同士に優劣や大小が存在する数値です。例えば、
      住宅価格といったようなものです。量的変数は連続値ともいいます。
    '''
  }
  {
    name: "Supervised learning.py"
    mode: "Python"
    content: '''
      #ロジスティッック回帰
      二値分類にも多クラス分類にも使うことができる分類方法です。
      広く使われている分類モデルの1つである。
      高い性能を発揮してくれるのは、線形分離可能なクラスに対してのみ。
      名前には「回帰」と入っていますが、分類のためのモデルなので気をつましょう。
      
      
      #SVM(サポートベクターマシン)
      二値分類の手法の一つです。
      2クラスと決定境界の間の距離を最大化することで、性能をあげる分類方法です。
      
      
      #K近傍法
      KNN(K Nearest Neighbor)とも呼ばれるK近傍法です。
      学習データを座標上にプロットし、未知のデータが与えられたら、
      その点から近い距離にあるデータ点をK個選ぶ。そのK個のデータ点で多数決をして、
      新しいデータ点のクラスを決めます。
      
      
      #決定木分析
      決定木による分類は、結界に至るまでの過程が容易に解釈ができる分類方法です。
      一連の質問に基づいて、決断を下すという方法で、データを分類していくモデルです。
      
      
      #ランダムフォレスト
      決定木を複数組み合わせて、各決定木の予測結果を多数決することで結果を導きます。
      そうすることによって、弱い学習アルゴリズムをより頑健な学習アルゴリズムに
      再構築することができます。
      
      
      #Deep Learning(ディープラーニング)
      Deep Learningとは、人間の力なしで機械が自動でデータから特徴を見つけてくれる
      ディープニューラルネットワーク(DNN)を用いた機械学習のことを指します。
      今までの機械学習と異なるのは、入力層と出力層の間にある、情報の伝達と処理を行う中間層が
      多層に存在する事です。Deep Learningは特徴量の精度や汎用性が非常に高い代わりに、
      実践には大量のデータが必要となることが特徴です。
    '''
  }
]
tags: []
isStarred: false
isTrashed: false
createdAt: "2017-09-21T12:50:39.312Z"
updatedAt: "2017-10-24T10:24:28.170Z"
